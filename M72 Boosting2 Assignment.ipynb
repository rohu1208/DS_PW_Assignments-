{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9bc4eab",
   "metadata": {},
   "source": [
    "# M72 Boosting2 Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b111d9",
   "metadata": {},
   "source": [
    "# 1. What is Gradient Boosting Regression?\n",
    "\n",
    "Gradient Boosting Regression is a machine learning technique for regression tasks that builds an ensemble of weak learners (usually decision trees) in a sequential manner. Each subsequent tree is trained to correct the errors made by the previous trees, using gradients of the loss function.## "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f10b45e",
   "metadata": {},
   "source": [
    "# Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96997048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.07829529694531265\n",
      "R-squared: 0.9511116376712166\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "class SimpleGradientBoostingRegressor:\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.models = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self.initial_prediction = np.mean(y)\n",
    "        residuals = y - self.initial_prediction\n",
    "        \n",
    "        for _ in range(self.n_estimators):\n",
    "            tree = DecisionTreeRegressor(max_depth=self.max_depth)\n",
    "            tree.fit(X, residuals)\n",
    "            predictions = tree.predict(X)\n",
    "            residuals -= self.learning_rate * predictions\n",
    "            self.models.append(tree)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        y_pred = np.full(X.shape[0], self.initial_prediction)\n",
    "        for model in self.models:\n",
    "            y_pred += self.learning_rate * model.predict(X)\n",
    "        return y_pred\n",
    "\n",
    "# Generate a simple regression dataset\n",
    "X = np.random.rand(100, 1)\n",
    "y = 4 * X.squeeze() + np.random.randn(100) * 0.5\n",
    "\n",
    "# Train the simple gradient boosting regressor\n",
    "gb = SimpleGradientBoostingRegressor(n_estimators=50, learning_rate=0.1, max_depth=3)\n",
    "gb.fit(X, y)\n",
    "y_pred = gb.predict(X)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y, y_pred)\n",
    "r2 = r2_score(y, y_pred)\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"R-squared: {r2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b875e1c",
   "metadata": {},
   "source": [
    "## Q3. Experiment with different hyperparameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f82fee81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 200}\n",
      "Mean Squared Error: 0.18601737824086217\n",
      "R-squared: 0.8791502836640005\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Generate a simple regression dataset\n",
    "X = np.random.rand(100, 1)\n",
    "y = 4 * X.squeeze() + np.random.randn(100) * 0.5\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 4, 5]\n",
    "}\n",
    "\n",
    "# Perform grid search\n",
    "gb = GradientBoostingRegressor()\n",
    "grid_search = GridSearchCV(estimator=gb, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Output the best parameters\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "\n",
    "# Evaluate the best model\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X)\n",
    "mse = mean_squared_error(y, y_pred)\n",
    "r2 = r2_score(y, y_pred)\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"R-squared: {r2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5cfc31",
   "metadata": {},
   "source": [
    "# Q4. What is a weak learner in Gradient Boosting?\n",
    "A weak learner in Gradient Boosting is a model that performs slightly better than random guessing. Typically, decision trees with limited depth (stumps) are used as weak learners.\n",
    "\n",
    "# Q5. What is the intuition behind the Gradient Boosting algorithm?\n",
    "The intuition behind Gradient Boosting is to sequentially add models that correct the errors of the combined ensemble of previous models. Each model is trained on the residual errors of the combined ensemble.\n",
    "\n",
    "# Q6. How does Gradient Boosting algorithm build an ensemble of weak learners?\n",
    "Gradient Boosting builds an ensemble of weak learners by sequentially adding new models trained on the residuals (errors) of the current ensemble's predictions. Each model attempts to reduce these residuals by fitting to the gradients of the loss function.\n",
    "\n",
    "# Q7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting algorithm?\n",
    "Initialize the model with a constant value (usually the mean of the target variable for regression).\n",
    "Compute residuals between the true values and the current model predictions.\n",
    "Fit a weak learner to the residuals.\n",
    "Update the model by adding the predictions of the weak learner, scaled by the learning rate.\n",
    "Repeat steps 2-4 for a specified number of iterations or until convergence.\n",
    "Combine the weak learners to make the final prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d56029",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
