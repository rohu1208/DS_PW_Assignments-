{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eeffb7b8",
   "metadata": {},
   "source": [
    "## ### Q1. What is boosting in machine learning?\n",
    "Boosting is an ensemble technique that combines multiple weak learners to form a strong learner, improving accuracy by focusing on misclassified instances.\n",
    "\n",
    "### Q2. What are the advantages and limitations of using boosting techniques?\n",
    "\n",
    "**Advantages:**\n",
    "- Improved accuracy\n",
    "- Robustness to overfitting\n",
    "- Flexibility\n",
    "- Model interpretability\n",
    "\n",
    "**Limitations:**\n",
    "- Computationally intensive\n",
    "- Sensitive to noisy data\n",
    "- Complexity\n",
    "\n",
    "### Q3. Explain how boosting works.\n",
    "Boosting sequentially trains weak learners, adjusting sample weights to focus on misclassified instances. The final model combines all weak learners, usually through weighted voting or averaging.\n",
    "\n",
    "### Q4. What are the different types of boosting algorithms?\n",
    "- AdaBoost\n",
    "- Gradient Boosting\n",
    "- XGBoost\n",
    "- LightGBM\n",
    "\n",
    "### Q5. What are some common parameters in boosting algorithms?\n",
    "- Number of estimators\n",
    "- Learning rate\n",
    "- Maximum depth of trees\n",
    "- Minimum samples split\n",
    "- Subsample ratio\n",
    "\n",
    "### Q6. How do boosting algorithms combine weak learners to create a strong learner?\n",
    "Boosting combines weak learners by assigning weights to their predictions, emphasizing learners that perform better and focusing on correcting errors from previous learners.\n",
    "\n",
    "### Q7. Explain the concept of the AdaBoost algorithm and its working.\n",
    "AdaBoost adjusts the weights of misclassified samples, training weak learners sequentially. Each learner is added with a weight proportional to its accuracy, focusing subsequent learners on harder cases.\n",
    "\n",
    "### Q8. What is the loss function used in the AdaBoost algorithm?\n",
    "AdaBoost uses an exponential loss function to penalize misclassified samples.\n",
    "\n",
    "### Q9. How does the AdaBoost algorithm update the weights of misclassified samples?\n",
    "AdaBoost increases the weights of misclassified samples, so the next weak learner focuses more on these harder cases.\n",
    "\n",
    "### Q10. What is the effect of increasing the number of estimators in the AdaBoost algorithm?\n",
    "Increasing the number of estimators can improve accuracy up to a point but may lead to overfitting if too many estimators are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513f81a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
