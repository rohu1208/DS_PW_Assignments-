{
 "cells": [
  {
   "cell_type": "raw",
   "id": "1b6deb19",
   "metadata": {},
   "source": [
    "Q1. Explain the concept of R-squared in linear regression models. How is it calculated, and what does it represent?\n",
    "R-squared,is a statistical measure used in linear regression to represent the proportion of the variance in the dependent variable that is predictable from the independent variable(s). It ranges from 0 to 1\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "52326e16",
   "metadata": {},
   "source": [
    "Q2. Define adjusted R-squared and explain how it differs from the regular R-squared.\n",
    "Adjusted R-squared adjusts the R^2 value based on the number of predictors in the model. It accounts for the model complexity by introducing a penalty for adding more predictors, which helps to avoid overestimating the explanatory power of the model."
   ]
  },
  {
   "cell_type": "raw",
   "id": "735bc97c",
   "metadata": {},
   "source": [
    "Q3. When is it more appropriate to use adjusted R-squared?\n",
    "Adjusted R-squared is more appropriate to use when comparing models with a different number of predictors. It provides a more reliable measure of model fit by accounting for the model complexity and penalizing the addition of non-significant predictors."
   ]
  },
  {
   "cell_type": "raw",
   "id": "89ba90f7",
   "metadata": {},
   "source": [
    "Q4. What are RMSE, MSE, and MAE in the context of regression analysis? How are these metrics calculated, and what do they represent?\n",
    "\n",
    "RMSE is the square root of the average of squared differences between observed and predicted values, giving higher weight to larger errors.\n",
    "MSE is the average of squared differences between observed and predicted values.\n",
    "MAE is the average of absolute differences between observed and predicted values."
   ]
  },
  {
   "cell_type": "raw",
   "id": "c2925eea",
   "metadata": {},
   "source": [
    "Q5. Discuss the advantages and disadvantages of using RMSE, MSE, and MAE as evaluation metrics in regression analysis.\n",
    "Advantages:\n",
    "\n",
    "RMSE and MSE are sensitive to outliers due to the squaring of errors, which can be useful if larger errors are particularly undesirable.\n",
    "MAE is more robust to outliers as it uses absolute differences, providing a more straightforward interpretation of average error magnitude.\n",
    "Disadvantages:\n",
    "\n",
    "RMSE and MSE can be overly influenced by outliers, potentially giving a misleading impression of model performance.\n",
    "MAE does not penalize larger errors as heavily as RMSE, which can be a drawback if large errors are particularly problematic."
   ]
  },
  {
   "cell_type": "raw",
   "id": "bc87208b",
   "metadata": {},
   "source": [
    "Q6. Explain the concept of Lasso regularization. How does it differ from Ridge regularization, and when is it more appropriate to use?\n",
    "Lasso Regularization (Least Absolute Shrinkage and Selection Operator):\n",
    "Lasso adds a penalty equal to the absolute value of the magnitude of coefficients to the loss function, encouraging sparsity (i.e., some coefficients being exactly zero, effectively performing variable selection).\n",
    "\n",
    "Ridge Regularization:\n",
    "Ridge adds a penalty equal to the square of the magnitude of coefficients to the loss function, shrinking coefficients but typically not setting any to zero.\n",
    "\n",
    "Differences:\n",
    "\n",
    "Lasso can set coefficients to zero, leading to simpler and more interpretable models.\n",
    "Ridge typically results in smaller coefficients but includes all predictors in the model.\n",
    "When to Use:\n",
    "\n",
    "Use Lasso when you believe that only a subset of the predictors are relevant and you want a more interpretable model.\n",
    "Use Ridge when you believe that most predictors contribute to the response and you want to handle multicollinearity."
   ]
  },
  {
   "cell_type": "raw",
   "id": "8deb26f9",
   "metadata": {},
   "source": [
    "Q7. How do regularized linear models help to prevent overfitting in machine learning? Provide an example to illustrate.\n",
    "Regularized linear models, such as Ridge and Lasso, add a penalty term to the loss function that discourages large coefficients, effectively reducing model complexity. This helps to prevent overfitting by avoiding overly flexible models that fit the training data too closely and perform poorly on unseen data.\n",
    "\n",
    "Example:\n",
    "Suppose we have a dataset with many predictors, some of which are highly correlated. A regular linear regression model might overfit by assigning large coefficients to these predictors. By applying Ridge regularization, the model will shrink the coefficients, reducing the variance and improving generalization to new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3784bcb4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
