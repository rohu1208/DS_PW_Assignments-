{
 "cells": [
  {
   "cell_type": "raw",
   "id": "932b8969",
   "metadata": {},
   "source": [
    "Q1. What is Lasso Regression, and how does it differ from other regression techniques?\n",
    "\n",
    "\n",
    "Lasso Regression (Least Absolute Shrinkage and Selection Operator) is a type of linear regression that includes a regularization term. It aims to enhance the model's prediction accuracy and interpretability by adding a penalty equal to the absolute value of the magnitude of coefficients (L1 norm)."
   ]
  },
  {
   "cell_type": "raw",
   "id": "0f752938",
   "metadata": {},
   "source": [
    "Q2. What is the main advantage of using Lasso Regression in feature selection?\n",
    "\n",
    "\n",
    "The main advantage of using Lasso Regression for feature selection is its ability to shrink some coefficients to exactly zero. This effectively removes less important features from the model, providing a sparse solution that identifies the most relevant predictors. This is particularly useful when dealing with high-dimensional data with many predictors."
   ]
  },
  {
   "cell_type": "raw",
   "id": "ed20f8c8",
   "metadata": {},
   "source": [
    "Q3. How do you interpret the coefficients of a Lasso Regression model?\n",
    "\n",
    "\n",
    "Interpreting the coefficients in Lasso Regression involves understanding that some coefficients may be exactly zero, indicating that the corresponding features are not important for predicting the response variable. The non-zero coefficients represent the strength and direction of the relationship between the predictors and the response variable. Due to the L1 penalty, the magnitude of the non-zero coefficients is typically reduced compared to OLS regression."
   ]
  },
  {
   "cell_type": "raw",
   "id": "031e08cb",
   "metadata": {},
   "source": [
    "Q4. What are the tuning parameters that can be adjusted in Lasso Regression, and how do they affect the model's performance?\n",
    "The primary tuning parameter in Lasso Regression is the regularization parameter \n",
    "𝜆\n",
    "λ. This parameter controls the strength of the penalty applied to the coefficients:\n",
    "\n",
    "\n",
    "λ=0: The model reduces to OLS regression with no regularization, potentially leading to overfitting.\n",
    "\n",
    "λ small: Results in small penalty, allowing more predictors to have non-zero coefficients.\n",
    "\n",
    "λ large: Increases the penalty, leading to more coefficients being shrunk to zero, thus increasing model sparsity and potentially preventing overfitting."
   ]
  },
  {
   "cell_type": "raw",
   "id": "56774411",
   "metadata": {},
   "source": [
    "Q5. Can Lasso Regression be used for non-linear regression problems? If yes, how?\n",
    "Lasso Regression itself is a linear technique, but it can be adapted for non-linear regression problems using the following approaches:\n",
    "\n",
    "Feature Engineering: Create polynomial or interaction terms from the original features to capture non-linear relationships, and then apply Lasso Regression on these transformed features.\n",
    "Kernel Methods: Use kernel tricks to transform the input space into a higher-dimensional space where linear relationships can capture non-linear patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176d4eaf",
   "metadata": {},
   "source": [
    "Q6. What is the difference between Ridge Regression and Lasso Regression?\n",
    "\n",
    "Ridge Regression: Uses an L2 norm penalty ,It shrinks coefficients but does not set any to zero, retaining all predictors in the model.\n",
    "\n",
    "Lasso Regression: Uses an L1 norm penalty,It can shrink coefficients to zero, effectively performing feature selection and providing a sparse model."
   ]
  },
  {
   "cell_type": "raw",
   "id": "60ab7b50",
   "metadata": {},
   "source": [
    "Can Lasso Regression handle multicollinearity in the input features? If yes, how?\n",
    "Yes, Lasso Regression can handle multicollinearity. The L1 penalty tends to select one predictor from a group of highly correlated predictors and shrink the others to zero. This can help in reducing the impact of multicollinearity by effectively selecting the most relevant predictor(s) from the correlated group, thus providing a more interpretable model."
   ]
  },
  {
   "cell_type": "raw",
   "id": "d27dcd86",
   "metadata": {},
   "source": [
    "Q8. How do you choose the optimal value of the regularization parameter (lambda) in Lasso Regression?\n",
    "The optimal value of the regularization parameter \n",
    "λ in Lasso Regression is typically chosen using cross-validation:\n",
    "\n",
    "Grid Search with Cross-Validation: Define a grid of possible values for \n",
    "λ.\n",
    "\n",
    "Cross-Validation: For each \n",
    "λ value, perform k-fold cross-validation and compute the validation error.\n",
    "\n",
    "Optimal λ: Choose the \n",
    "λ that results in the lowest cross-validation error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fb040d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
